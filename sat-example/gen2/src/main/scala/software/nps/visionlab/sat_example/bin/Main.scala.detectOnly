package software.nps.visionlab.sat_example.bin

import org.apache.spark.SparkContext
import org.apache.spark.SparkConf
import org.apache.spark.SparkFiles
import org.apache.spark.sql.SQLContext
import org.apache.spark.sql.Row
import org.apache.spark.sql.types.StructType
import org.apache.spark.sql.types.StructField
import org.apache.spark.sql.types.StringType
import org.apache.hadoop.fs.FileSystem
import org.apache.hadoop.fs.Path
import org.apache.hadoop.mapred.JobConf
import org.apache.hadoop.io.ArrayWritable
import org.apache.hadoop.io.BytesWritable
import org.apache.hadoop.io.NullWritable
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat
import org.apache.hadoop.mapreduce.Job
import org.apache.commons.io.FileUtils
import org.apache.commons.io.filefilter.SuffixFileFilter
import org.apache.commons.io.filefilter.WildcardFileFilter

import software.uncharted.salt.core.projection.numeric._
import software.uncharted.salt.core.generation.request._
import software.uncharted.salt.core.generation.Series
import software.uncharted.salt.core.generation.TileGenerator
import software.uncharted.salt.core.generation.output.SeriesData
import software.uncharted.salt.core.analytic.numeric._

import java.io._
import java.awt.geom.Point2D
import java.awt.geom.Rectangle2D

import scala.util.parsing.json.JSONObject
import scala.collection.JavaConversions._
import scala.collection.mutable.ArrayBuffer
import org.gdal.gdal.Dataset



object Main {

  // Defines the tile size in both x and y bin dimensions
  val tileSize = 256

  // Defines the output layer name
  val layerName = "satData"

  def printToFile(f: java.io.File)(op: java.io.PrintWriter => Unit) {
      val p = new java.io.PrintWriter(f)
      try {
          op(p)
      } finally {
          p.close()
      }
  }

  // Creates and returns an Array of Double values encoded as 64bit Integers
  def createByteBuffer(tile: SeriesData[(Int, Int, Int), (Int, Int), Double, (Double, Double)]): Array[Byte] = {
    val byteArray = new Array[Byte](tileSize * tileSize * 8)
    var j = 0
    tile.bins.foreach(b => {
      val data = java.lang.Double.doubleToLongBits(b)
      for (i <- 0 to 7) {
        byteArray(j) = ((data >> (i * 8)) & 0xff).asInstanceOf[Byte]
        j += 1
      }
    })
    byteArray
  }


  def main(args: Array[String]): Unit = {
    if (args.length < 2) {
      println("Requires commandline: <spark-submit command> inputFilePath outputPath")
      System.exit(-1)
    }

    val inputPathName = args(0)
    val outputPath = args(1)

    println("Removing old tiles " + outputPath + "/" + layerName)
    var layerPath = new File(outputPath + "/" + layerName)
    FileUtils.deleteDirectory(layerPath)
    layerPath.mkdirs()

    val conf = new SparkConf().setAppName("sat-example")
    conf.set("spark.kryoserializer.buffer.max", "1000MB")
    conf.set("spark.executor.memory", "6GB")
    conf.set("spark.driver.maxResultSize", "6GB")
    conf.set("spark.executor.extraLibraryPath", "/home/trbatcha/tools/lib")
    conf.set("spark.executor.extrajavaoptions", "-XX:+UseConcMarkSweepGC")
    conf.set("log4j.logger.org.apache.spark.rpc.akka.ErrorMonitor", "FATAL")
    val sc = new SparkContext(conf)
    val hadoopConf = sc.hadoopConfiguration
    //val hjob = new Job(hadoopConf);
    val hjob = Job.getInstance(hadoopConf)
    val fs = org.apache.hadoop.fs.FileSystem.get(hadoopConf)
    val inputPath = new Path(inputPathName)
    val exists = fs.exists(inputPath)
    if (!exists){
        println( inputPathName + " does not exist!!")
        System.exit(1)
    }else {
        println("Processing " + inputPathName)
    }
    // Don't hadd the path since we are adding individual files
    //FileInputFormat.setInputPaths(hjob, inputPath)
    var remoteIter = fs.listFiles(inputPath, true)
    //debug
    var count = 0
    var max_count = 1000
    var done = false
    while (remoteIter.hasNext() && done == false) {
        if (count > max_count && max_count > 0)
            done = true
        else {
            var f = remoteIter.next()
            println("next " + f.getPath().toString())
            if (f.isFile()){
                val name = f.getPath().getName()
                var ext = name.split('.').drop(1).lastOption
                ext = ext.map(_.toLowerCase)
                if (ext == Some("ntf") || ext == Some("tif")) {
                    count += 1
                    println("adding file " + name)
                    FileInputFormat.addInputPath(hjob, f.getPath())
                }
            }
        }
    }
    val detectorScript = "/home/trbatcha/salt-examples/sat-example/doDetect.py"
    sc.addFile(detectorScript)
    val modelfile = "/home/trbatcha/salt-examples/sat-example/mymodel.caffemodel"
    sc.addFile(modelfile)
    val protofile = "/home/trbatcha/salt-examples/sat-example/test_ships.prototxt"
    sc.addFile(protofile)
    val cfgfile = "/home/trbatcha/salt-examples/sat-example/faster_rcnn_end2end_ships.yml"
    sc.addFile(cfgfile)
    //Add all the python files from py-faster-rcnn
    //FileUtils.listFiles(new File("/home/trbatcha/salt-examples/sat-example/lib"), 
    //               Array("py"),true).foreach( f =>
    //              {
    //                 val fullPath = f.getPath()
    //                val newPath = fullPath.replace(
    //                  "/home/trbatcha/salt-examples/sat-example/", "")
    //              println("copying " + newPath)
    //             sc.addFile(newPath)
    //        })

    val resData = sc.newAPIHadoopRDD(hjob.getConfiguration(),
                  classOf[WholeFileInputFormat],
                  classOf[String],classOf[ArrayWritable]).flatMap(n => {
                    val fname = n._1
                    println(fname)
                    var data = n._2
                    var bytes = data.get().asInstanceOf[Array[BytesWritable]]
                    println("array bytes length " + bytes.length)
                    var out = None: Option[FileOutputStream]
                    val tname = "/dev/shm/" + fname
                    var bcnt = 0
                    try {
                        out = Some(new FileOutputStream(tname))
                        for (i <- 0 until bytes.length) {
                            val next = bytes(i).copyBytes()
                            bcnt += next.length
                            out.get.write(next)
                        }
                    } catch {
                        case e: IOException => e.printStackTrace
                    } finally {
                        if (out.isDefined)
                            out.get.close
                    }
                    println("wrote temp file of size " + bcnt)
                    var nfile = new File(tname)
                    var haveCorner = false
                    var points : Array[java.awt.geom.Point2D] = 
                        Array(new java.awt.geom.Point2D.Double(0.0, 0.0), 
                              new java.awt.geom.Point2D.Double(0.0, 0.0), 
                              new java.awt.geom.Point2D.Double(0.0, 0.0), 
                              new java.awt.geom.Point2D.Double(0.0, 0.0)) 
                    var props = None : Option[java.util.Properties]
                    //var mres : collection.mutable.Seq[Row] = 
                    //                       collection.mutable.Seq()
                    var mres : ArrayBuffer[Row] = ArrayBuffer()
                    try {
                        if (bcnt > 0) {
                            var gfoot = new  GDALFootprint()
                            val npoints = gfoot.getCorners(nfile)
                            if (npoints != null) {
                                haveCorner = true
                                points = npoints
                            }
                            props = Some(gfoot.getLastProps())
                            val dscript = SparkFiles.get(detectorScript)
                            val mfile = SparkFiles.get(modelfile)
                            val pfile = SparkFiles.get(protofile)
                            val cfg = SparkFiles.get(cfgfile)
                            val dir = SparkFiles.getRootDirectory()
                            println("files dir: " + dir)
                            val detects: 
                             java.util.ArrayList
                                     [GDALFootprint.DetectionResult] = 
                                    gfoot.getDetections(dir + "/doDetect.py", 
                                                 dir + "/mymodel.caffemodel",
                                                 dir + "/test_ships.prototxt",
                                                 dir + "/faster_rcnn_end2end_ships.yml")
                            println("detects size " + String.valueOf(detects.size));
                            for (i <- 0 until detects.size) {
                                mres +=  Row(fname, detects.get(i).objName, 
                                    String.valueOf(detects.get(i).p0.getX()),
                                    String.valueOf(detects.get(i).p0.getY()),
                                    String.valueOf(detects.get(i).p1.getX()),
                                    String.valueOf(detects.get(i).p1.getY()),
                                    String.valueOf(detects.get(i).p2.getX()),
                                    String.valueOf(detects.get(i).p2.getY()),
                                    String.valueOf(detects.get(i).p3.getX()),
                                    String.valueOf(detects.get(i).p3.getY()))
                            }
                            gfoot.CloseDataset()
                        }
                    } catch {
                        case e: IllegalArgumentException => {}
                    } finally {
                        nfile.delete()
                    }
                    println(fname)
                    //println(mres(0)) 
                    if (haveCorner)
                        mres += Row(fname, "corner", 
                               String.valueOf(points(0).getX()), 
                               String.valueOf(points(0).getY()),
                               String.valueOf(points(1).getX()), 
                               String.valueOf(points(1).getY()),
                               String.valueOf(points(2).getX()), 
                               String.valueOf(points(2).getY()),
                               String.valueOf(points(3).getX()), 
                               String.valueOf(points(3).getY()))
                    //println(mres(1)) 
                   
                    println("size of res " + String.valueOf(mres.length))
                    mres
                   // }).persist()
                    }).collect()
    }
}
